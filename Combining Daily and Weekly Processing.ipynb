{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed519aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Data...\n",
      "Step 2: Processing Weekly Features...\n",
      "   Data merged. Shape: (1229, 468)\n",
      "Step 3: Calculating PCA Components and Elastic Net Scores...\n",
      "   Attached 4 PCA components for Fundamental\n",
      "   Attached ENet Score for Fundamental (Selected 14/17 features)\n",
      "   Attached 5 PCA components for Technical\n",
      "   Attached ENet Score for Technical (Selected 10/14 features)\n",
      "   Attached 5 PCA components for EIA\n",
      "   Attached ENet Score for EIA (Selected 0/406 features)\n",
      "Step 4: Saving final dataset to /Users/arthurf/Desktop/INDENG 242A/final_daily_processed_data.csv...\n",
      "Done! File is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def process_full_oil_data(input_file='/Users/arthurf/Desktop/INDENG 242A/shortened_oil_data.csv', output_file='/Users/arthurf/Desktop/INDENG 242A/final_daily_processed_data.csv'):\n",
    "    print(\"Step 1: Loading Data...\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_file} not found. Please upload the file.\")\n",
    "        return\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # Step 2: Weekly Feature Processing (Logic from Analysis_Weekly_Features_v2.ipynb)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Step 2: Processing Weekly Features...\")\n",
    "    \n",
    "    # Identify Daily vs Weekly columns based on naming conventions\n",
    "    daily_cols_potential = [\n",
    "        'Brent_Oil', 'USD_CAD', 'Crude_Oil', 'DXY', 'Emerging_Mkt', 'Gold', \n",
    "        'Copper', 'Heating_Oil', 'Junk_Bond', 'Transportation', 'Natural_Gas',\n",
    "        'Oil_Services', 'Gasoline', 'Inflation_Tips', 'Energy_Stocks', 'SP500', \n",
    "        'Oil_VIX', 'US10Y', 'Crack_Spread_321', 'Gold_Oil_Ratio', 'Copper_Oil_Ratio',\n",
    "        'Transport_Oil_Ratio', 'Service_Oil_Ratio',\n",
    "        'SMA_5', 'SMA_20', 'SMA_50', 'Dist_SMA_20', 'Dist_SMA_50',\n",
    "        'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position', \n",
    "        'RSI_14', 'RSI_28', 'MACD', 'MACD_Signal', 'MACD_Hist',\n",
    "        'Ret_1d', 'Ret_5d', 'Ret_20d', 'Ret_60d',\n",
    "        'Realized_Vol_20d', 'Realized_Vol_60d', 'Annual_Vol_20d',\n",
    "        'Vol_Adj_Mom_20d', 'Vol_Adj_Mom_60d',\n",
    "        'Price_Range_20d', 'ROC_5', 'ROC_20',\n",
    "        'Oil_Lag1', 'Oil_Lag2', 'Oil_Lag3', 'Oil_Lag5',\n",
    "        'RSI_14_Lag1', 'MACD_Hist_Lag1', 'Ret_1d_Lag1', 'Ret_1d_Lag2',\n",
    "        'US_Stocks_Crude', 'US_Stocks_Ex_SPR_Crude', 'US_Stocks_Crude_SPR'\n",
    "    ]\n",
    "    \n",
    "    weekly_cols = [col for col in df.columns if '4W_' in col or col.startswith('US_Stocks_') \n",
    "                   or col.startswith('US_') or col.startswith('PADD') or col.startswith('AK_')\n",
    "                   or col.startswith('L48_') or col.startswith('New_') or col.startswith('Central_')\n",
    "                   or col.startswith('Lower_') or col.startswith('Cushing_')]\n",
    "    \n",
    "    # Refine lists based on overlap\n",
    "    weekly_cols = [col for col in weekly_cols if col not in daily_cols_potential]\n",
    "    daily_cols = [col for col in daily_cols_potential if col in df.columns]\n",
    "    \n",
    "    # Create Week Identifier\n",
    "    df['Year_Week'] = df['Date'].dt.isocalendar().year.astype(str) + '_W' + df['Date'].dt.isocalendar().week.astype(str).str.zfill(2)\n",
    "    \n",
    "    # Aggregate to Weekly: Daily cols take last value, Weekly cols take filled last value\n",
    "    # We perform the aggregation logic\n",
    "    def agg_logic(x):\n",
    "        data = {}\n",
    "        # For daily columns, take the last available value in the week\n",
    "        for col in daily_cols:\n",
    "            if col in x.columns:\n",
    "                data[col] = x[col].iloc[-1]\n",
    "        \n",
    "        # For weekly columns, use backfill then take last to ensure data availability\n",
    "        for col in weekly_cols:\n",
    "            if col in x.columns:\n",
    "                val = x[col].bfill().iloc[-1] if x[col].notna().any() else np.nan\n",
    "                data[col] = val\n",
    "        return pd.Series(data)\n",
    "\n",
    "    df_weekly_agg = df.groupby('Year_Week').apply(agg_logic).reset_index()\n",
    "    \n",
    "    # Merge aggregated weekly values back to daily dataframe\n",
    "    # This ensures \"in the same week every day will be the same\" for weekly vars\n",
    "    \n",
    "    # First, drop original weekly columns from df to avoid duplicates/confusion\n",
    "    df_clean = df[['Date', 'Year_Week'] + daily_cols].copy()\n",
    "    \n",
    "    # Merge the aggregated weekly features\n",
    "    # Suffix '_WeeklyAgg' is not needed if we replace, but to be safe we rename columns from aggregation\n",
    "    # to differentiate or just overwrite. The prompt implies \"attached\", so let's overwrite the weekly columns with their weekly-constant versions.\n",
    "    df_final = pd.merge(df_clean, df_weekly_agg.drop(columns=daily_cols, errors='ignore'), on='Year_Week', how='left')\n",
    "    \n",
    "    # Handle NaNs (Imputation)\n",
    "    # Filter numeric columns for imputation\n",
    "    numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_final[numeric_cols] = imputer.fit_transform(df_final[numeric_cols])\n",
    "    \n",
    "    print(f\"   Data merged. Shape: {df_final.shape}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Step 3: Define Feature Groups (from PCA.ipynb & ENet.ipynb)\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    # Fundamental\n",
    "    fund_cols = df_final.columns[1:24].tolist() # Approximate from notebook logic\n",
    "    # Refining based on explicit list if available in snippets or intersection\n",
    "    fundamental_cols = [c for c in [\n",
    "        'Brent_Oil','USD_CAD','DXY','Emerging_Mkt','Gold','Copper','Heating_Oil',\n",
    "        'Junk_Bond','Transportation','Natural_Gas','Oil_Services','Gasoline',\n",
    "        'Inflation_Tips','Energy_Stocks','SP500','Oil_VIX','US10Y'\n",
    "    ] if c in df_final.columns]\n",
    "\n",
    "    # Technical (Momentum + Volatility)\n",
    "    mom_features = [f for f in ['RSI_14', 'RSI_28', 'ROC_5', 'ROC_20', 'Ret_5d', 'Ret_20d', 'Ret_60d',\n",
    "                                'Vol_Adj_Mom_20d', 'Vol_Adj_Mom_60d'] if f in df_final.columns]\n",
    "    vol_features = [f for f in ['BB_Width', 'Realized_Vol_20d', 'Realized_Vol_60d', 'Annual_Vol_20d', 'Price_Range_20d'] if f in df_final.columns]\n",
    "    tech_cols = mom_features + vol_features\n",
    "    \n",
    "    # EIA Weekly\n",
    "    eia_cols = [col for col in weekly_cols if col in df_final.columns]\n",
    "    \n",
    "    feature_groups = {\n",
    "        'Fundamental': fundamental_cols,\n",
    "        'Technical': tech_cols,\n",
    "        'EIA': eia_cols\n",
    "    }\n",
    "    \n",
    "    # Target for Elastic Net (Using Crude Oil Price as per ENet notebook, or Returns)\n",
    "    # Notebook ENet (1).ipynb uses: y = df['Crude_Oil']\n",
    "    y = df_final['Crude_Oil']\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Step 4 & 5: Apply PCA and Elastic Net to Groups\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Step 3: Calculating PCA Components and Elastic Net Scores...\")\n",
    "\n",
    "    for name, cols in feature_groups.items():\n",
    "        if len(cols) == 0:\n",
    "            continue\n",
    "            \n",
    "        X_group = df_final[cols]\n",
    "        \n",
    "        # --- PCA ---\n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_group)\n",
    "        \n",
    "        # Fit PCA\n",
    "        pca = PCA(n_components=0.90) # Keep components explaining 90% variance\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Attach PCs to dataframe\n",
    "        n_pcs = X_pca.shape[1]\n",
    "        # Limit to top 5 to avoid bloating if 90% requires many, or keep all as requested \"pca resulting pcs\"\n",
    "        # We will keep up to 5 for clarity, or all if fewer.\n",
    "        n_pcs_to_keep = min(n_pcs, 5) \n",
    "        \n",
    "        for i in range(n_pcs_to_keep):\n",
    "            df_final[f'PCA_{name}_PC{i+1}'] = X_pca[:, i]\n",
    "            \n",
    "        print(f\"   Attached {n_pcs_to_keep} PCA components for {name}\")\n",
    "\n",
    "        # --- Elastic Net ---\n",
    "        # The user wants \"enet resulting stuff\". We will calculate the ENet Prediction/Score\n",
    "        # This creates a single synthetic feature representing the group's predictive power.\n",
    "        try:\n",
    "            enet_cv = ElasticNetCV(cv=3, random_state=42, max_iter=2000)\n",
    "            enet_cv.fit(X_scaled, y)\n",
    "            \n",
    "            # Predict (Score)\n",
    "            enet_score = enet_cv.predict(X_scaled)\n",
    "            df_final[f'ENet_{name}_Score'] = enet_score\n",
    "            \n",
    "            # Also identifying selected features could be useful, but adding them as columns (flags) might be messy.\n",
    "            # The Score is the most useful dense \"derived\" attribute.\n",
    "            selected_feats = np.array(cols)[enet_cv.coef_ != 0]\n",
    "            print(f\"   Attached ENet Score for {name} (Selected {len(selected_feats)}/{len(cols)} features)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Skipping ENet for {name} due to error: {e}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Step 6: Save\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"Step 4: Saving final dataset to {output_file}...\")\n",
    "    df_final.to_csv(output_file, index=False)\n",
    "    print(\"Done! File is ready.\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a dummy csv if not exists for demonstration or expect user file\n",
    "    # This block assumes the environment has the file.\n",
    "    process_full_oil_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
