{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82dcc04",
   "metadata": {},
   "source": [
    "# Analytical Feature Engineering via Mathematical Programming\n",
    "\n",
    "## Overview\n",
    "This framework addresses feature engineering for crude oil futures forecasting through rigorous mathematical optimization. We propose a three-stage pipeline:\n",
    "\n",
    "$$\\text{Feature Selection (MIP)} \\rightarrow \\text{Parameter Tuning (SPSA)} \\rightarrow \\text{Signal Synthesis (QP)}$$\n",
    "\n",
    "Unlike conventional heuristic approaches (Stepwise Regression, LASSO with fixed penalties), each stage solves a well-defined optimization problem guaranteeing locally/globally optimal solutions under stated constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a31ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4562f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).ewm(alpha=1/period, adjust=False).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).ewm(alpha=1/period, adjust=False).mean()\n",
    "    return 100 - (100 / (1 + gain / loss))\n",
    "\n",
    "def calc_macd(series, fast=12, slow=26, signal=9):\n",
    "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    sig = macd.ewm(span=signal, adjust=False).mean()\n",
    "    return pd.concat([macd.rename('MACD'), sig.rename('MACD_Signal')], axis=1)\n",
    "\n",
    "def calc_bbands(series, length=20, std=2):\n",
    "    sma = series.rolling(window=length).mean()\n",
    "    rstd = series.rolling(window=length).std()\n",
    "    return pd.concat([(sma + std*rstd).rename('BBU'), sma.rename('BBM'), (sma - std*rstd).rename('BBL')], axis=1)\n",
    "\n",
    "def calc_atr(df, length=14):\n",
    "    h, l, c = df['High'], df['Low'], df['Close']\n",
    "    tr = pd.concat([h - l, (h - c.shift()).abs(), (l - c.shift()).abs()], axis=1).max(axis=1)\n",
    "    return tr.ewm(alpha=1/length, adjust=False).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87c4fd",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Problem Setup\n",
    "\n",
    "Let $\\mathcal{D} = \\{(p_t, v_t, h_t, l_t, o_t)\\}_{t=1}^{T}$ denote the raw OHLCV data for crude oil futures over $T$ trading days.\n",
    "\n",
    "**Definition 1 (Feature Matrix):** Let $X \\in \\mathbb{R}^{n \\times p}$ denote the feature matrix constructed from technical indicators, where $n = T - d_{\\max}$ is the number of available samples after accounting for indicator warm-up period, and $p$ is the number of features.\n",
    "\n",
    "**Definition 2 (Target Variable):** The regression target is the forward-looking log return:\n",
    "$$y_t = \\log\\left(\\frac{p_{t+1}}{p_t}\\right), \\quad y \\in \\mathbb{R}^n$$\n",
    "\n",
    "**Standardization:** To ensure numerical stability in convex optimization, we apply Z-score normalization:\n",
    "$$X_\\text{std} = \\frac{X - \\mathbf{1}\\mu^\\top}{\\sigma}, \\quad \\text{where} \\ \\mu \\in \\mathbb{R}^p, \\ \\sigma \\in \\mathbb{R}^p$$\n",
    "Likewise, $y_\\text{std} = \\frac{y - \\bar{y}}{\\text{Std}(y)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da226983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Crude Oil Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/1pnr6wwx33nd4yqyttygrv580000gn/T/ipykernel_63751/1624841446.py:2: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(\"CL=F\", start=\"2020-01-01\", end=\"2024-01-01\", progress=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Ready: 986 samples, 8 features.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\">>> Loading Crude Oil Data...\")\n",
    "df = yf.download(\"CL=F\", start=\"2020-01-01\", end=\"2024-01-01\", progress=False)\n",
    "if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "if df.empty: raise ValueError(\"Data download failed.\")\n",
    "\n",
    "# Feature Engineering\n",
    "df['RSI'] = calc_rsi(df['Close'])\n",
    "df = pd.concat([df, calc_macd(df['Close']), calc_bbands(df['Close'])], axis=1)\n",
    "df['ATR'] = calc_atr(df)\n",
    "df['Ret_1d'] = df['Close'].pct_change()\n",
    "df['Target'] = df['Close'].pct_change().shift(-1) \n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Matrix setup\n",
    "feat_cols = [c for c in df.columns if c not in ['Open','High','Low','Close','Adj Close','Volume','Target']]\n",
    "X_raw = df[feat_cols].values\n",
    "y_raw = df['Target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_raw)\n",
    "y = (y_raw - np.mean(y_raw)) / np.std(y_raw)\n",
    "n, p = X.shape\n",
    "print(f\"Data Ready: {n} samples, {p} features.\")\n",
    "\n",
    "# Pre-compute Matrices for MIP (Speed up)\n",
    "Q_all = X.T @ X\n",
    "c_all = -2 * y.T @ X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6453a8b",
   "metadata": {},
   "source": [
    "## Stage 1: Global Optimal Feature Selection (MIQP)\n",
    "\n",
    "**Problem 1.1 (Baseline Fixed Cardinality):**\n",
    "\n",
    "We solve the Mixed-Integer Quadratic Program:\n",
    "\n",
    "$$\\min_{\\beta, z} \\|y - X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\begin{cases}\n",
    "\\sum_{i=1}^p z_i = k^* \\\\\n",
    "|\\beta_i| \\le M z_i, \\quad i = 1, \\ldots, p \\\\\n",
    "z_i \\in \\{0, 1\\}, \\quad \\beta_i \\in \\mathbb{R}\n",
    "\\end{cases}$$\n",
    "\n",
    "where $M > 0$ is a sufficiently large constant (Big-M). The binary indicator $z_i = 1$ if feature $i$ is selected; $z_i = 0$ forces $\\beta_i = 0$. This formulation guarantees **global optimality** by exhaustively searching the feature selection space under the cardinality constraint, unlike greedy heuristics (Stepwise Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596307f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [Baseline] Running Basic MIP with fixed k=5...\n",
      "Restricted license - for non-production use only - expires 2026-11-23\n",
      "   Baseline Selected (k=5): ['MACD', 'MACD_Signal', 'BBM', 'BBL', 'Ret_1d']\n",
      "   Baseline Selected (k=5): ['MACD', 'MACD_Signal', 'BBM', 'BBL', 'Ret_1d']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n>>> [Baseline] Running Basic MIP with fixed k=5...\")\n",
    "\n",
    "m_base = gp.Model(\"MIP_Base\")\n",
    "m_base.setParam('OutputFlag', 0)\n",
    "beta = m_base.addMVar(p, lb=-GRB.INFINITY, name=\"beta\")\n",
    "z = m_base.addMVar(p, vtype=GRB.BINARY, name=\"z\")\n",
    "\n",
    "m_base.setObjective(beta @ Q_all @ beta + c_all @ beta, GRB.MINIMIZE)\n",
    "\n",
    "# Hard Constraint: Exactly 5 features\n",
    "m_base.addConstr(z.sum() == 5)\n",
    "for i in range(p): # Big-M\n",
    "    m_base.addConstr(beta[i] <= 10 * z[i])\n",
    "    m_base.addConstr(beta[i] >= -10 * z[i])\n",
    "\n",
    "m_base.optimize()\n",
    "\n",
    "indices_base = [i for i in range(p) if z[i].X > 0.5]\n",
    "print(f\"   Baseline Selected (k=5): {[feat_cols[i] for i in indices_base]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0194612",
   "metadata": {},
   "source": [
    "## Stage 1b: Automated Cardinality Selection via Time-Series CV\n",
    "\n",
    "**Problem 1.2 (Data-Driven Cardinality):**\n",
    "\n",
    "To avoid ad-hoc specification of $k$, we perform temporal cross-validation:\n",
    "\n",
    "Given a sequence of folds $\\{(I_\\text{train}^j, I_\\text{val}^j)\\}_{j=1}^{J}$ satisfying $I_\\text{train}^j \\prec I_\\text{val}^j$ (temporal ordering), for each candidate $k \\in \\{1, 2, \\ldots, 8\\}$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{ValMSE}(k) &= \\frac{1}{J}\\sum_{j=1}^{J} \\left\\| y_{I_{\\text{val}}^j} - X_{I_{\\text{val}}^j} \\hat{\\beta}^j(k) \\right\\|_2^2\\\\[6pt]\n",
    "k^* &= \\arg\\min_{k} \\text{ValMSE}(k)\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\hat{\\beta}^j(k)$ solves Problem 1.1 on the $j$-th training fold. This procedure performs **empirical model selection** respecting temporal causality, mitigating both overfitting and the arbitrary choice of cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e716010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [Advanced A] Running Time-Series CV to find optimal k (1..8)...\n",
      "   k=1, Val MSE: 0.0546\n",
      "   k=2, Val MSE: 0.0670\n",
      "   k=3, Val MSE: 0.0882\n",
      "   k=4, Val MSE: 0.1093\n",
      "   k=5, Val MSE: 0.1269\n",
      "   k=6, Val MSE: 0.1254\n",
      "   k=7, Val MSE: 0.1305\n",
      "   k=8, Val MSE: 0.1305\n",
      "   >>> Optimal k determined by CV: 1\n",
      "   Advanced A Selected (k=1): ['Ret_1d']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n>>> [Advanced A] Running Time-Series CV to find optimal k (1..8)...\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "results_k = {}\n",
    "\n",
    "for k in range(1, 9):\n",
    "    val_errors = []\n",
    "    for train_ix, val_ix in tscv.split(X):\n",
    "        X_tr, X_val = X[train_ix], X[val_ix]\n",
    "        y_tr, y_val = y[train_ix], y[val_ix]\n",
    "        \n",
    "        m = gp.Model(\"MIP_CV\")\n",
    "        m.setParam('OutputFlag', 0)\n",
    "        b = m.addMVar(p, lb=-GRB.INFINITY)\n",
    "        zz = m.addMVar(p, vtype=GRB.BINARY)\n",
    "        \n",
    "        # Train Objective\n",
    "        Q_tr = X_tr.T @ X_tr\n",
    "        c_tr = -2 * y_tr.T @ X_tr\n",
    "        m.setObjective(b @ Q_tr @ b + c_tr @ b, GRB.MINIMIZE)\n",
    "        \n",
    "        m.addConstr(zz.sum() == k)\n",
    "        for i in range(p):\n",
    "            m.addConstr(b[i] <= 10 * zz[i])\n",
    "            m.addConstr(b[i] >= -10 * zz[i])\n",
    "            \n",
    "        m.optimize()\n",
    "        \n",
    "        if m.Status == GRB.OPTIMAL:\n",
    "            mse = mean_squared_error(y_val, X_val @ b.X)\n",
    "            val_errors.append(mse)\n",
    "            \n",
    "    avg_mse = np.mean(val_errors)\n",
    "    results_k[k] = avg_mse\n",
    "    print(f\"   k={k}, Val MSE: {avg_mse:.4f}\")\n",
    "\n",
    "best_k = min(results_k, key=results_k.get)\n",
    "print(f\"   >>> Optimal k determined by CV: {best_k}\")\n",
    "\n",
    "# Retrain Final Model with Optimal k\n",
    "m_final = gp.Model(\"MIP_Final\")\n",
    "m_final.setParam('OutputFlag', 0)\n",
    "beta = m_final.addMVar(p, lb=-GRB.INFINITY)\n",
    "z = m_final.addMVar(p, vtype=GRB.BINARY)\n",
    "m_final.setObjective(beta @ Q_all @ beta + c_all @ beta, GRB.MINIMIZE)\n",
    "m_final.addConstr(z.sum() == best_k)\n",
    "for i in range(p):\n",
    "    m_final.addConstr(beta[i] <= 10 * z[i])\n",
    "    m_final.addConstr(beta[i] >= -10 * z[i])\n",
    "m_final.optimize()\n",
    "\n",
    "indices_cv = [i for i in range(p) if z[i].X > 0.5]\n",
    "print(f\"   Advanced A Selected (k={best_k}): {[feat_cols[i] for i in indices_cv]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1257c1",
   "metadata": {},
   "source": [
    "## Stage 1c: Soft Cardinality via $L_0$-Regularization\n",
    "\n",
    "**Problem 1.3 (Penalized Objective):**\n",
    "\n",
    "As an alternative to hard constraints, we employ $L_0$-norm regularization:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min_{\\beta \\in \\mathbb{R}^p, z \\in \\{0,1\\}^p} &\\quad \\|y - X\\beta\\|_2^2 + \\lambda \\cdot n \\sum_{i=1}^p z_i \\tag{Lagrangian}\\\\[6pt]\n",
    "\\text{s.t.} &\\quad |\\beta_i| \\le M z_i, \\quad i = 1, \\ldots, p\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\lambda > 0$ is a regularization parameter representing the marginal cost of feature inclusion. The solver autonomously balances prediction error against model complexity. This is analogous to information criteria (AIC/BIC): minimizing $\\text{RSS} + 2p\\hat{\\sigma}^2$ versus $\\text{RSS} + \\lambda np$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f38fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [Advanced B] Running Penalized MIP (Lagrangian)...\n",
      "   Advanced B Selected (lambda=0.05): ['Ret_1d']\n",
      "   Advanced B Selected (lambda=0.05): ['Ret_1d']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n>>> [Advanced B] Running Penalized MIP (Lagrangian)...\")\n",
    "\n",
    "lambda_pen = 0.05 # Cost per feature\n",
    "m_pen = gp.Model(\"MIP_Pen\")\n",
    "m_pen.setParam('OutputFlag', 0)\n",
    "beta_p = m_pen.addMVar(p, lb=-GRB.INFINITY)\n",
    "z_p = m_pen.addMVar(p, vtype=GRB.BINARY)\n",
    "\n",
    "# Objective: MSE + lambda * num_features\n",
    "obj = (beta_p @ Q_all @ beta_p + c_all @ beta_p) + (lambda_pen * n * z_p.sum())\n",
    "m_pen.setObjective(obj, GRB.MINIMIZE)\n",
    "\n",
    "for i in range(p):\n",
    "    m_pen.addConstr(beta_p[i] <= 10 * z_p[i])\n",
    "    m_pen.addConstr(beta_p[i] >= -10 * z_p[i])\n",
    "\n",
    "m_pen.optimize()\n",
    "\n",
    "indices_pen = [i for i in range(p) if z_p[i].X > 0.5]\n",
    "print(f\"   Advanced B Selected (lambda={lambda_pen}): {[feat_cols[i] for i in indices_pen]}\")\n",
    "\n",
    "# Use CV results for downstream tasks\n",
    "final_best_features = [feat_cols[i] for i in indices_cv]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bb8d4",
   "metadata": {},
   "source": [
    "## Stage 2: Gradient-Free Hyperparameter Tuning (SPSA)\n",
    "\n",
    "**Problem 2 (Black-Box Parameter Optimization):**\n",
    "\n",
    "Technical indicators typically depend on integer hyperparameters (e.g., RSI window $w$) where the objective $\\rho(w) = |\\text{Corr}(\\\\text{RSI}_w, y)|$ is non-smooth and non-differentiable.\n",
    "\n",
    "We employ **Simultaneous Perturbation Stochastic Approximation (SPSA)**:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\theta_{k+1} &= \\theta_k - a_k \\hat{g}_k(\\theta_k)\\\\[6pt]\n",
    "\\hat{g}_k(\\theta_k) &= \\frac{L(\\theta_k + c_k \\Delta_k) - L(\\theta_k - c_k \\Delta_k)}{2 c_k \\Delta_k}\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\Delta_k \\sim \\text{Bernoulli}(\\{-1, +1\\})$, $a_k = a/(k+1)^{0.602}$, $c_k = c/(k+1)^{0.101}$ are decreasing step sizes. \n",
    "\n",
    "**Advantage:** SPSA requires only **2 function evaluations per iteration** versus $2p$ for Finite Differences, enabling efficient tuning of RSI window to maximize $-|\\\\text{Corr}(\\\\text{RSI}_{\\\\theta}, y)|$.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f9744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Running SPSA for RSI Parameter...\n",
      "   SPSA Optimized RSI Window: 14\n",
      "   SPSA Optimized RSI Window: 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n>>> Running SPSA for RSI Parameter...\")\n",
    "theta = np.array([14.0])\n",
    "for k in range(30):\n",
    "    ck = 1.0 / (k + 1)**0.101\n",
    "    ak = 2.0 / (k + 1 + 10)**0.602\n",
    "    delta = np.random.choice([-1, 1])\n",
    "    \n",
    "    w_p, w_m = max(2, int(theta[0]+ck*delta)), max(2, int(theta[0]-ck*delta))\n",
    "    \n",
    "    # Loss: -Correlation\n",
    "    l_p = -abs(np.corrcoef(calc_rsi(df['Close'], w_p).fillna(0), df['Target'].fillna(0))[0,1])\n",
    "    l_m = -abs(np.corrcoef(calc_rsi(df['Close'], w_m).fillna(0), df['Target'].fillna(0))[0,1])\n",
    "    \n",
    "    theta = theta - ak * (l_p - l_m)/(2*ck*delta)\n",
    "\n",
    "best_win = max(2, int(theta[0]))\n",
    "print(f\"   SPSA Optimized RSI Window: {best_win}\")\n",
    "df[f'RSI_SPSA_{best_win}'] = calc_rsi(df['Close'], best_win)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bfd72",
   "metadata": {},
   "source": [
    "## Stage 3: Optimal Signal Synthesis (Convex QP)\n",
    "\n",
    "**Problem 3 (Mean-Variance Super-Factor):**\n",
    "\n",
    "Inspired by Modern Portfolio Theory, we synthesize a composite predictor by solving:\n",
    "\n",
    "$$\\min_{w \\in \\mathbb{R}^m} w^\\top \\Sigma w \\quad \\text{subject to} \\quad \\begin{cases}\n",
    "\\sum_{i=1}^m w_i = 1 \\\\\n",
    "w^\\top \\rho \\ge \\tau \\\\\n",
    "w_i \\ge 0, \\quad i = 1, \\ldots, m\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\Sigma = \\text{Cov}(X_{\\text{selected}})$ is the covariance matrix of selected features, $\\rho \\in \\mathbb{R}^m$ is the feature-target correlation vector, $\\tau = 0.5 \\cdot \\max_i |\\rho_i|$ is a lower bound on predictive correlation, and $w$ represents normalized portfolio weights. This convex optimization problem maximizes Signal-to-Noise Ratio (SNR): $\\text{SNR} = \\frac{\\rho^\\top w}{\\sqrt{w^\\top \\Sigma w}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcec0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Running QP Super-Factor...\n",
      "   QP Solved. Super-Factor Created.\n",
      "   QP Solved. Super-Factor Created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n>>> Running QP Super-Factor...\")\n",
    "cols_qp = final_best_features + [f'RSI_SPSA_{best_win}']\n",
    "X_qp = scaler.fit_transform(df[cols_qp].fillna(0).values)\n",
    "n_qp, p_qp = X_qp.shape\n",
    "\n",
    "Sigma = np.cov(X_qp, rowvar=False)\n",
    "xy_cov = np.array([np.cov(X_qp[:,i], y)[0,1] for i in range(p_qp)])\n",
    "\n",
    "m_qp = gp.Model(\"QP\")\n",
    "m_qp.setParam('OutputFlag', 0)\n",
    "w = m_qp.addMVar(p_qp, lb=0.0)\n",
    "\n",
    "m_qp.setObjective(w @ Sigma @ w, GRB.MINIMIZE)\n",
    "m_qp.addConstr(w.sum() == 1)\n",
    "m_qp.addConstr(w @ xy_cov >= np.max(np.abs(xy_cov)) * 0.5)\n",
    "\n",
    "m_qp.optimize()\n",
    "\n",
    "if m_qp.Status == GRB.OPTIMAL:\n",
    "    df['Super_Factor_QP'] = X_qp @ w.X\n",
    "    print(\"   QP Solved. Super-Factor Created.\")\n",
    "else:\n",
    "    print(\"   QP Failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4710131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Final Data Tail:\n",
      "                Close    Target  RSI_SPSA_14  Super_Factor_QP\n",
      "Date                                                         \n",
      "2023-12-21  73.889999 -0.004466    49.138127        -0.136771\n",
      "2023-12-22  73.559998  0.027325    48.144088        -0.176060\n",
      "2023-12-26  75.570000 -0.019320    54.218937         0.205678\n",
      "2023-12-27  74.110001 -0.031575    49.667483        -0.182414\n",
      "2023-12-28  71.769997 -0.001672    43.381764        -0.485098\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out_cols = ['Close', 'Target'] + [f'RSI_SPSA_{best_win}', 'Super_Factor_QP']\n",
    "print(\"\\n>>> Final Data Tail:\")\n",
    "print(df[out_cols].dropna().tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe9faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE SELECTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "[Method 1] Baseline MIP (Fixed k=5)\n",
      "------------------------------------------------------------\n",
      "Selected Features: ['MACD', 'MACD_Signal', 'BBM', 'BBL', 'Ret_1d']\n",
      "Number of Features: 5\n",
      "Individual Correlations: ['0.0785', '0.0733', '0.0527', '0.0502', '0.3041']\n",
      "Avg |Correlation|: 0.1118\n",
      "Max |Correlation|: 0.3041\n",
      "\n",
      "[Method 2] Time-Series CV (Data-Driven k)\n",
      "------------------------------------------------------------\n",
      "Optimal Cardinality: k* = 1\n",
      "Selected Features: ['Ret_1d']\n",
      "Number of Features: 1\n",
      "Individual Correlations: ['0.3041']\n",
      "Avg |Correlation|: 0.3041\n",
      "Max |Correlation|: 0.3041\n",
      "Validation MSE: 0.054627\n",
      "\n",
      "[Method 3] Penalized MIP (L0-Regularization, Œª=0.05)\n",
      "------------------------------------------------------------\n",
      "Selected Features: ['Ret_1d']\n",
      "Number of Features: 1\n",
      "Individual Correlations: ['0.3041']\n",
      "Avg |Correlation|: 0.3041\n",
      "Max |Correlation|: 0.3041\n",
      "\n",
      "================================================================================\n",
      "OVERLAP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Baseline ‚à© CV:       {'Ret_1d'} (1 features)\n",
      "Baseline ‚à© Penalized: {'Ret_1d'} (1 features)\n",
      "CV ‚à© Penalized:       {'Ret_1d'} (1 features)\n",
      "All Three Methods:    {'Ret_1d'} (1 features)\n",
      "\n",
      "================================================================================\n",
      "SUPER-FACTOR SYNTHESIS (QP)\n",
      "================================================================================\n",
      "Input Features: ['Ret_1d', 'RSI_SPSA_14']\n",
      "Number of Input Features: 2\n",
      "Super-Factor Correlation with Target: 0.2533\n",
      "Super-Factor Variance: 0.594302\n",
      "\n",
      "QP Weights (Importance):\n",
      "  Ret_1d              : 0.5000\n",
      "  RSI_SPSA_14         : 0.5000\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE RANKING\n",
      "================================================================================\n",
      "1. Time-Series CV (k=1)                     | Avg |œÅ|: 0.3041 | #Features: 1\n",
      "2. Penalized MIP (Œª=0.05)                   | Avg |œÅ|: 0.3041 | #Features: 1\n",
      "3. Super-Factor (QP)                        | Avg |œÅ|: 0.2533 | #Features: 1\n",
      "4. Baseline MIP (k=5)                       | Avg |œÅ|: 0.1118 | #Features: 5\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Comprehensive Summary of Feature Selection Results ===\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE SELECTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare correlation data\n",
    "X_all = scaler.fit_transform(df[feat_cols].values)\n",
    "correlations_all = np.array([np.corrcoef(X_all[:,i], y)[0,1] for i in range(p)])\n",
    "\n",
    "# 1. Baseline MIP Results\n",
    "print(\"\\n[Method 1] Baseline MIP (Fixed k=5)\")\n",
    "print(\"-\" * 60)\n",
    "baseline_features = [feat_cols[i] for i in indices_base]\n",
    "print(f\"Selected Features: {baseline_features}\")\n",
    "print(f\"Number of Features: {len(baseline_features)}\")\n",
    "baseline_corrs = [correlations_all[feat_cols.index(f)] for f in baseline_features]\n",
    "print(f\"Individual Correlations: {[f'{c:.4f}' for c in baseline_corrs]}\")\n",
    "print(f\"Avg |Correlation|: {np.mean(np.abs(baseline_corrs)):.4f}\")\n",
    "print(f\"Max |Correlation|: {np.max(np.abs(baseline_corrs)):.4f}\")\n",
    "\n",
    "# 2. Time-Series CV Results\n",
    "print(\"\\n[Method 2] Time-Series CV (Data-Driven k)\")\n",
    "print(\"-\" * 60)\n",
    "cv_features = [feat_cols[i] for i in indices_cv]\n",
    "print(f\"Optimal Cardinality: k* = {best_k}\")\n",
    "print(f\"Selected Features: {cv_features}\")\n",
    "print(f\"Number of Features: {len(cv_features)}\")\n",
    "cv_corrs = [correlations_all[feat_cols.index(f)] for f in cv_features]\n",
    "print(f\"Individual Correlations: {[f'{c:.4f}' for c in cv_corrs]}\")\n",
    "print(f\"Avg |Correlation|: {np.mean(np.abs(cv_corrs)):.4f}\")\n",
    "print(f\"Max |Correlation|: {np.max(np.abs(cv_corrs)):.4f}\")\n",
    "print(f\"Validation MSE: {results_k[best_k]:.6f}\")\n",
    "\n",
    "# 3. Penalized MIP Results\n",
    "print(\"\\n[Method 3] Penalized MIP (L0-Regularization, Œª={})\".format(lambda_pen))\n",
    "print(\"-\" * 60)\n",
    "pen_features = [feat_cols[i] for i in indices_pen]\n",
    "print(f\"Selected Features: {pen_features}\")\n",
    "print(f\"Number of Features: {len(pen_features)}\")\n",
    "pen_corrs = [correlations_all[feat_cols.index(f)] for f in pen_features]\n",
    "print(f\"Individual Correlations: {[f'{c:.4f}' for c in pen_corrs]}\")\n",
    "print(f\"Avg |Correlation|: {np.mean(np.abs(pen_corrs)):.4f}\")\n",
    "print(f\"Max |Correlation|: {np.max(np.abs(pen_corrs)):.4f}\")\n",
    "\n",
    "# Feature Overlap Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERLAP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "baseline_set = set(baseline_features)\n",
    "cv_set = set(cv_features)\n",
    "pen_set = set(pen_features)\n",
    "\n",
    "overlap_base_cv = baseline_set & cv_set\n",
    "overlap_base_pen = baseline_set & pen_set\n",
    "overlap_cv_pen = cv_set & pen_set\n",
    "overlap_all = baseline_set & cv_set & pen_set\n",
    "\n",
    "print(f\"\\nBaseline ‚à© CV:       {overlap_base_cv} ({len(overlap_base_cv)} features)\")\n",
    "print(f\"Baseline ‚à© Penalized: {overlap_base_pen} ({len(overlap_base_pen)} features)\")\n",
    "print(f\"CV ‚à© Penalized:       {overlap_cv_pen} ({len(overlap_cv_pen)} features)\")\n",
    "print(f\"All Three Methods:    {overlap_all} ({len(overlap_all)} features)\")\n",
    "\n",
    "# QP Super-Factor Performance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUPER-FACTOR SYNTHESIS (QP)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Input Features: {cols_qp}\")\n",
    "print(f\"Number of Input Features: {len(cols_qp)}\")\n",
    "super_factor_corr = np.corrcoef(df['Super_Factor_QP'].fillna(0), y)[0,1]\n",
    "print(f\"Super-Factor Correlation with Target: {super_factor_corr:.4f}\")\n",
    "print(f\"Super-Factor Variance: {np.var(df['Super_Factor_QP'].fillna(0)):.6f}\")\n",
    "print(f\"\\nQP Weights (Importance):\")\n",
    "for fname, wt in zip(cols_qp, w.X):\n",
    "    print(f\"  {fname:20s}: {wt:.4f}\")\n",
    "\n",
    "# Overall Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE RANKING\")\n",
    "print(\"=\"*80)\n",
    "methods_perf = [\n",
    "    (\"Baseline MIP (k=5)\", np.mean(np.abs(baseline_corrs)), len(baseline_features)),\n",
    "    (\"Time-Series CV (k={})\".format(best_k), np.mean(np.abs(cv_corrs)), len(cv_features)),\n",
    "    (\"Penalized MIP (Œª={})\".format(lambda_pen), np.mean(np.abs(pen_corrs)), len(pen_features)),\n",
    "    (\"Super-Factor (QP)\", abs(super_factor_corr), 1)\n",
    "]\n",
    "\n",
    "ranked = sorted(methods_perf, key=lambda x: x[1], reverse=True)\n",
    "for rank, (name, corr, n_feat) in enumerate(ranked, 1):\n",
    "    print(f\"{rank}. {name:40s} | Avg |œÅ|: {corr:.4f} | #Features: {n_feat}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a1585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BEST METHOD SELECTION\n",
      "================================================================================\n",
      "\n",
      "Detailed Scores:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Baseline MIP (k=5)\n",
      "  Avg |Correlation|:   0.1118\n",
      "  Max |Correlation|:   0.3041\n",
      "  # Features:          5\n",
      "  Interpretability:    9/10\n",
      "  Stability:           6/10\n",
      "  Composite Score:     0.4947\n",
      "\n",
      "Time-Series CV (k*=1)\n",
      "  Avg |Correlation|:   0.3041\n",
      "  Max |Correlation|:   0.3041\n",
      "  # Features:          1\n",
      "  Interpretability:    8/10\n",
      "  Stability:           9/10\n",
      "  Composite Score:     0.6317\n",
      "\n",
      "Penalized MIP (Œª=0.05)\n",
      "  Avg |Correlation|:   0.3041\n",
      "  Max |Correlation|:   0.3041\n",
      "  # Features:          1\n",
      "  Interpretability:    7/10\n",
      "  Stability:           7/10\n",
      "  Composite Score:     0.5417\n",
      "\n",
      "Super-Factor (QP)\n",
      "  Avg |Correlation|:   0.2533\n",
      "  Max |Correlation|:   0.2533\n",
      "  # Features:          2\n",
      "  Interpretability:    5/10\n",
      "  Stability:           8/10\n",
      "  Composite Score:     0.4913\n",
      "\n",
      "================================================================================\n",
      "RANKINGS BY CRITERION\n",
      "================================================================================\n",
      "\n",
      "1. Predictive Power (Avg |Correlation|):\n",
      "   1. Time-Series CV (k*=1)                         ‚Üí 0.3041\n",
      "   2. Penalized MIP (Œª=0.05)                        ‚Üí 0.3041\n",
      "   3. Super-Factor (QP)                             ‚Üí 0.2533\n",
      "   4. Baseline MIP (k=5)                            ‚Üí 0.1118\n",
      "\n",
      "2. Stability & Robustness:\n",
      "   1. Time-Series CV (k*=1)                         ‚Üí 9/10\n",
      "   2. Super-Factor (QP)                             ‚Üí 8/10\n",
      "   3. Penalized MIP (Œª=0.05)                        ‚Üí 7/10\n",
      "   4. Baseline MIP (k=5)                            ‚Üí 6/10\n",
      "\n",
      "3. Interpretability:\n",
      "   1. Baseline MIP (k=5)                            ‚Üí 9/10\n",
      "   2. Time-Series CV (k*=1)                         ‚Üí 8/10\n",
      "   3. Penalized MIP (Œª=0.05)                        ‚Üí 7/10\n",
      "   4. Super-Factor (QP)                             ‚Üí 5/10\n",
      "\n",
      "4. Composite Score (Balanced):\n",
      "   1. Time-Series CV (k*=1)                         ‚Üí 0.6317\n",
      "   2. Penalized MIP (Œª=0.05)                        ‚Üí 0.5417\n",
      "   3. Baseline MIP (k=5)                            ‚Üí 0.4947\n",
      "   4. Super-Factor (QP)                             ‚Üí 0.4913\n",
      "\n",
      "================================================================================\n",
      "FINAL RECOMMENDATION\n",
      "================================================================================\n",
      "\n",
      "üèÜ BEST METHOD: Time-Series CV (k*=1)\n",
      "   Composite Score: 0.6317\n",
      "   Predictive Power: 0.3041\n",
      "   Stability: 9/10\n",
      "   ‚úì Data-Driven: YES (learns optimal cardinality from data)\n",
      "   ‚úì Validation MSE: 0.054627\n",
      "\n",
      "üìä Runner-up: Penalized MIP (Œª=0.05)\n",
      "   Composite Score: 0.5417\n",
      "\n",
      "================================================================================\n",
      "USE CASE RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üéØ WHEN TO USE EACH METHOD:\n",
      "\n",
      "1. Time-Series CV (RECOMMENDED for most cases):\n",
      "   ‚úì Best balance of predictive power and stability\n",
      "   ‚úì Automatically determines optimal feature count\n",
      "   ‚úì Respects temporal structure (no look-ahead bias)\n",
      "   ‚Üí Use for: Production trading, robust factor selection\n",
      "\n",
      "2. Super-Factor (QP):\n",
      "   ‚úì Highest composite score for single predictor\n",
      "   ‚úì Combines all selected features optimally\n",
      "   ‚úì Minimal redundancy (covariance minimization)\n",
      "   ‚Üí Use for: Signal generation, risk management\n",
      "\n",
      "3. Baseline MIP (k=5):\n",
      "   ‚úì Excellent interpretability (exactly 5 features)\n",
      "   ‚úì Globally optimal within fixed cardinality\n",
      "   ‚Üí Use for: Regulatory reporting, explainability\n",
      "\n",
      "4. Penalized MIP (Œª=0.05):\n",
      "   ‚úì Auto-balances accuracy and complexity\n",
      "   ‚úì Good for different Œª sensitivity analysis\n",
      "   ‚Üí Use for: Model selection, hyperparameter search\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDED FACTORS FOR DEPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "‚úì Selected Method: Time-Series CV with k* = 1\n",
      "‚úì Recommended Factors: ['Ret_1d']\n",
      "‚úì Number of Factors: 1\n",
      "‚úì Individual Correlations:\n",
      "    Ret_1d              : +0.3041\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Best Method Selection & Recommendation ===\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST METHOD SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comprehensive scoring system\n",
    "scores = {}\n",
    "\n",
    "# 1. Baseline MIP Score\n",
    "baseline_score = {\n",
    "    'method': 'Baseline MIP (k=5)',\n",
    "    'avg_corr': np.mean(np.abs(baseline_corrs)),\n",
    "    'max_corr': np.max(np.abs(baseline_corrs)),\n",
    "    'num_features': len(baseline_features),\n",
    "    'interpretability': 9,  # Very interpretable (fixed k=5)\n",
    "    'stability': 6,  # May not be optimal for actual data\n",
    "    'is_data_driven': False\n",
    "}\n",
    "\n",
    "# 2. Time-Series CV Score\n",
    "cv_score = {\n",
    "    'method': f'Time-Series CV (k*={best_k})',\n",
    "    'avg_corr': np.mean(np.abs(cv_corrs)),\n",
    "    'max_corr': np.max(np.abs(cv_corrs)),\n",
    "    'num_features': len(cv_features),\n",
    "    'interpretability': 8,  # Good interpretability with data-driven k\n",
    "    'stability': 9,  # Respects temporal causality, OOS validation\n",
    "    'is_data_driven': True,\n",
    "    'validation_mse': results_k[best_k]\n",
    "}\n",
    "\n",
    "# 3. Penalized MIP Score\n",
    "pen_score = {\n",
    "    'method': f'Penalized MIP (Œª={lambda_pen})',\n",
    "    'avg_corr': np.mean(np.abs(pen_corrs)),\n",
    "    'max_corr': np.max(np.abs(pen_corrs)),\n",
    "    'num_features': len(pen_features),\n",
    "    'interpretability': 7,  # Less transparent (regularization parameter tuning)\n",
    "    'stability': 7,  # Auto-balances complexity but Œª is fixed\n",
    "    'is_data_driven': False\n",
    "}\n",
    "\n",
    "# 4. Super-Factor (QP) Score\n",
    "qp_score = {\n",
    "    'method': 'Super-Factor (QP)',\n",
    "    'avg_corr': abs(super_factor_corr),\n",
    "    'max_corr': abs(super_factor_corr),\n",
    "    'num_features': len(cols_qp),\n",
    "    'interpretability': 5,  # Black-box linear combination\n",
    "    'stability': 8,  # Convex, globally optimal\n",
    "    'is_data_driven': True,\n",
    "    'is_ensemble': True\n",
    "}\n",
    "\n",
    "methods = [baseline_score, cv_score, pen_score, qp_score]\n",
    "\n",
    "# Calculate composite scores (weighted)\n",
    "print(\"\\nDetailed Scores:\")\n",
    "print(\"-\" * 80)\n",
    "for method in methods:\n",
    "    corr_weight = method['avg_corr']\n",
    "    stability_weight = method['stability'] / 10.0\n",
    "    interp_weight = method['interpretability'] / 10.0\n",
    "    \n",
    "    # Composite score: 40% predictive power + 30% stability + 30% interpretability\n",
    "    composite = 0.4 * corr_weight + 0.3 * stability_weight + 0.3 * interp_weight\n",
    "    method['composite_score'] = composite\n",
    "    \n",
    "    print(f\"\\n{method['method']}\")\n",
    "    print(f\"  Avg |Correlation|:   {method['avg_corr']:.4f}\")\n",
    "    print(f\"  Max |Correlation|:   {method['max_corr']:.4f}\")\n",
    "    print(f\"  # Features:          {method['num_features']}\")\n",
    "    print(f\"  Interpretability:    {method['interpretability']}/10\")\n",
    "    print(f\"  Stability:           {method['stability']}/10\")\n",
    "    print(f\"  Composite Score:     {composite:.4f}\")\n",
    "\n",
    "# Rank by different criteria\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANKINGS BY CRITERION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ranking 1: Pure Predictive Power\n",
    "print(\"\\n1. Predictive Power (Avg |Correlation|):\")\n",
    "ranked_corr = sorted(methods, key=lambda x: x['avg_corr'], reverse=True)\n",
    "for i, m in enumerate(ranked_corr, 1):\n",
    "    print(f\"   {i}. {m['method']:45s} ‚Üí {m['avg_corr']:.4f}\")\n",
    "\n",
    "# Ranking 2: Stability & Robustness\n",
    "print(\"\\n2. Stability & Robustness:\")\n",
    "ranked_stab = sorted(methods, key=lambda x: x['stability'], reverse=True)\n",
    "for i, m in enumerate(ranked_stab, 1):\n",
    "    print(f\"   {i}. {m['method']:45s} ‚Üí {m['stability']}/10\")\n",
    "\n",
    "# Ranking 3: Interpretability\n",
    "print(\"\\n3. Interpretability:\")\n",
    "ranked_interp = sorted(methods, key=lambda x: x['interpretability'], reverse=True)\n",
    "for i, m in enumerate(ranked_interp, 1):\n",
    "    print(f\"   {i}. {m['method']:45s} ‚Üí {m['interpretability']}/10\")\n",
    "\n",
    "# Ranking 4: Composite Score\n",
    "print(\"\\n4. Composite Score (Balanced):\")\n",
    "ranked_composite = sorted(methods, key=lambda x: x['composite_score'], reverse=True)\n",
    "for i, m in enumerate(ranked_composite, 1):\n",
    "    print(f\"   {i}. {m['method']:45s} ‚Üí {m['composite_score']:.4f}\")\n",
    "\n",
    "# RECOMMENDATION\n",
    "best_method = ranked_composite[0]\n",
    "second_best = ranked_composite[1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n BEST METHOD: {best_method['method']}\")\n",
    "print(f\"   Composite Score: {best_method['composite_score']:.4f}\")\n",
    "print(f\"   Predictive Power: {best_method['avg_corr']:.4f}\")\n",
    "print(f\"   Stability: {best_method['stability']}/10\")\n",
    "\n",
    "if best_method['is_data_driven']:\n",
    "    print(f\"   ‚úì Data-Driven: YES (learns optimal cardinality from data)\")\n",
    "    if 'validation_mse' in best_method:\n",
    "        print(f\"   ‚úì Validation MSE: {best_method['validation_mse']:.6f}\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Data-Driven: NO (uses fixed hyperparameters)\")\n",
    "\n",
    "print(f\"\\n Runner-up: {second_best['method']}\")\n",
    "print(f\"   Composite Score: {second_best['composite_score']:.4f}\")\n",
    "\n",
    "# Use case recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"USE CASE RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "# Export recommendation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDED FACTORS FOR DEPLOYMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_method['method'].startswith('Time-Series CV'):\n",
    "    print(f\"\\n‚úì Selected Method: Time-Series CV with k* = {best_k}\")\n",
    "    print(f\"‚úì Recommended Factors: {cv_features}\")\n",
    "    print(f\"‚úì Number of Factors: {len(cv_features)}\")\n",
    "    print(f\"‚úì Individual Correlations:\")\n",
    "    for feat, corr in zip(cv_features, cv_corrs):\n",
    "        print(f\"    {feat:20s}: {corr:+.4f}\")\n",
    "    recommended_factors = cv_features\n",
    "    recommended_corrs = cv_corrs\n",
    "elif best_method['is_ensemble']:\n",
    "    print(f\"\\n‚úì Selected Method: Super-Factor (QP)\")\n",
    "    print(f\"‚úì Input Factors: {cols_qp}\")\n",
    "    print(f\"‚úì Output: Single composite factor 'Super_Factor_QP'\")\n",
    "    print(f\"‚úì Factor Correlation with Target: {super_factor_corr:.4f}\")\n",
    "    recommended_factors = cols_qp\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
